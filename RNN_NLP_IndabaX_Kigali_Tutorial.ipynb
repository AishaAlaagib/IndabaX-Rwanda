{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_NLP_IndabaX_Kigali_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "CLb3FfKrX6WD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "from IPython.display import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ao7A0uxtAL7d",
        "colab_type": "code",
        "outputId": "62fed056-37e1-4610-83f5-de045440901e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QD6_TWPtTDVp",
        "colab_type": "code",
        "outputId": "28b23f63-a614-40f7-8a03-83a9fba8352e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.9.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vfFm3Ey4DbUS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 0 : Introduction to RNN and GRU"
      ]
    },
    {
      "metadata": {
        "id": "3gdAoaqCsR70",
        "colab_type": "code",
        "outputId": "0dc08bf7-7c8c-4534-b8a9-6f5a554348f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "Image(\"RNN_Diagram.png\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "RNN_Diagram.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "2u8tCEHquXsM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One on the main drawback of FFNN is their lack of ability to manage sequential data since for this kind of model, the features are invariant par permutation . RNN address this issue by using a memory cell $h_t$ that is modified every time it sees a new input $x_t$ in order to produce a time-dependent output $y_t$. For instance, for an input sequence $(x_0,x_1,x_2)$, instead of having $y = FFNN( [x_0,x_1,x_2] )$, The RNN will behave as follow :\n",
        "\n",
        "$y_1, h_1 = RNN(x_0)$ ;\n",
        "\n",
        "$y_2, h_2 = RNN(x_1)$ ;\n",
        "\n",
        "$y_3, h_3 = RNN(x_2)$ etc...\n",
        "\n",
        "The $x_t$ can be words in a sentence for example"
      ]
    },
    {
      "metadata": {
        "id": "G634PFDbuMj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "  \n",
        "Mathematically, recurrent neural net can be written as:  \n",
        "\n",
        "$H_t = \\phi(U*X_t + W*H_{t-1})$\n",
        "\n",
        "$y_t = \\phi(V*H_{t})$\n",
        "\n",
        "U: weight matrix associated to the $x_t$\n",
        "\n",
        "W:  weight matrix associated to the  hidden layer\n",
        "\n",
        "V: weight matrix associated to the output layer\n",
        "\n",
        "The hidden state at time step t is $h_t$. It is a function of the input at the same time step $x_t$ and  the hidden state of the previous time step $ h_{t-1}$ The function  $\\phi$ can be the sigmoid function($\\sigma$) or tangent hyperbolic($tanh$)\n",
        "\n",
        "$y_t$ represente the output at time step t"
      ]
    },
    {
      "metadata": {
        "id": "ovVcvPDQvhk1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Intuitively**, we as humans processes sequences in a similar way:\n",
        "\n",
        "1)At each time step $t$, we update our memory (what we know) $h_t$ based on what we previously knew $h_{t-1}$ and the new information received $x_t$. Therefore, we can write $h_t = f(x_t, h_{t-1})$\n",
        "\n",
        "2) The information that we output may not have the same structure / size as what is inside our memory : we transform the information in our memory to produce a desired output : $y_t = g(h_t)$\n",
        "\n",
        "In RNN, for $f$ and $g$, we simply apply affine transformations to the inputs $x_t$ : $(U*X_t$), $h_{t-1}$ : ($W*H_{t-1}$) and $h_t$  : ($V*H_{t}$)to interpret them in a \"convenient way\" (w.r.t the task) and to make sure that dimensions are aligned before passing them to an activation function ($tanh$ or sigmoid)"
      ]
    },
    {
      "metadata": {
        "id": "y3TW3OMTxvsD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Limitations of RNN:\n",
        "\n",
        "During back propagation, recurrent neural networks suffer from the vanishing  gradient problem which means when we will update the model's parameters, the gradient will be very close to 0.\n",
        "\n",
        "more about backpropagation and vanishing gradient: http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/,  https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257\n"
      ]
    },
    {
      "metadata": {
        "id": "l3mwI-Fyx-lv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GRU\n",
        "\n",
        "GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. \n",
        "\n",
        "To solve that problem , GRU uses, so called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output.\n",
        "\n",
        "## Update Gate:\n",
        "\n",
        "$z_t = \\sigma(W^{(z)}x_t + U^{(z)}h_{t-1})$\n",
        "\n",
        "$x_t$:  current input\n",
        "\n",
        "$W^{(z)}$ : weight associated to $x_t$\n",
        "\n",
        "$h_{t-1}$ : output of the previous unit\n",
        "\n",
        "$U^{(z)}$: weight associated to $h_{t-1}$\n",
        "\n",
        "The update gate helps the model to determine how much of the past information  needs to be passed along to the future. \n",
        "Suppose we have: \n",
        "### 'Hello, my name is Jean  and I'm from France. I speak....'\n",
        "\n",
        "To predict the next word we don't need to remember all the previous part. We can just use 'I'm from France' to now that the word to predict is 'French'. That's exactly what the update gate does, it allow the network to jus remember the usefull part to predict the next output.\n",
        "\n",
        "That allows the model to decide to copy all the information from the past and eliminate the risk of vanishing gradient problem\n",
        "\n",
        "## Reset Gate:\n",
        "\n",
        "$r_t = \\sigma(W^{(r)}x_t + U^{(r)}h_{t-1})$\n",
        "\n",
        "this gate is used from the model to decide how much of the past information to forget. The difference  between Update Gate and Reset Gate comes in the weights and the gate’s usage.\n",
        "\n",
        "Coming back to the example above the reset gate tell to the model you don't need to remember  'Hello, my name is Jean  and' to predict the next word.\n",
        "\n",
        "## Current memory content\n",
        "\n",
        "This will use the reset gate to store the relevant information from the past.\n",
        "\n",
        "$h_{t}^{\\prime} = \\tanh(Wx_t +r_t ⊙  Uh_{t-1})$ \n",
        "\n",
        "$r_t ⊙  Uh_{t-1} $:  Hadamard (element-wise) product between the reset gate $r_t$ and $Uh_{t-1}$. That will determine what to remove from the previous time steps\n",
        "\n",
        "$\\tanh$: nonlinear activation function\n",
        "\n",
        "## Final memory at current time step:\n",
        "\n",
        "In this step the update gate is used.  It determines what to collect from the current memory content  $h_{t}^{\\prime}$ and what from the previous steps  $ h_{t-1}$\n",
        "\n",
        "$h_t = z_t  ⊙ h_{t-1} + (1-z_t)  ⊙ h_{t}^{\\prime}$\n",
        "\n",
        "* Apply element-wise multiplication to the update gate $z_t$ and $h_{t-1}$.\n",
        "* Apply element-wise multiplication to $(1-z_t) $and $h_{t}^{\\prime}$.\n",
        "* Sum the two results \n",
        "\n",
        "\n",
        "Putting all in a graph GRU looks like the schema below:"
      ]
    },
    {
      "metadata": {
        "id": "JyThQ4kXyJIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "more details [towardsdatascience](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be), [medium](https://medium.com/ai-journal/lstm-gru-recurrent-neural-networks-81fe2bcdf1f9), [karpathy blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "more references:\n",
        "\n",
        "https://skymind.ai/wiki/lstm\n",
        "\n",
        "https://medium.com/@carynmccarthy15/a-beginners-guide-to-recurrent-neural-networks-bfacb27bddb6\n",
        "\n",
        "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
        "\n",
        "https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82\n",
        "\n",
        "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
        "\n",
        "https://towardsdatascience.com/what-is-a-recurrent-nns-and-gated-recurrent-unit-grus-ea71d2a05a69\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kQjdaHwbMn3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1 : Question Answering with ParlAI"
      ]
    },
    {
      "metadata": {
        "id": "wqgtkAD_yZPC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Question Answering \n",
        "\n",
        "![Question answering](https://github.com/AishaAlaagib/IndabaX-Rwanda/blob/master/fa4_fig01.jpg)\n",
        "\n",
        "Search engines like Google and Yahoo! that allow the users to search for documents on the World\n",
        "Wide Web. In Search engines the user has to check each and every document to get useful answer to the\n",
        "question and it is a time consuming process. **The Question Answering (QA) system** reduces the search time toget exact answer to the question. Question Answering system is an important research area in information\n",
        "retrieval. Research on the area of Question Answering system started in the year 1960 and present lot of\n",
        "Question Answering systems have been developed. Question Answering system combines the research from\n",
        "different domains like Natural Language Processing, Artificial Intelligence, Information Retrieval and\n",
        "Information extraction.\n",
        "\n",
        "**The objective** of question answering system is to find exact answer to the question asked\n",
        "by user in natural language\n",
        "\n",
        "To understand the Question Answering subject, we firstly define the associated terms. \n",
        "\n",
        "*   **A Question Phrase** is the part of the question that says what is searched.\n",
        "*   ** Question Type**  refers to a categorization of the question for its purpose.\n",
        "*   ** Answer Type** refers to a class of objects which are sought by the question. \n",
        "*    **Question Focus** is the property or entity being searched by the question. \n",
        "*   **Question Topic** is the object or event that the question is about. \n",
        "*  **Candidate Passage** can broadly be defined as anything from a sentence to a document retrieved by a search engine in response to a question. \n",
        "* ** Candidate Answer** is the text ranked according to its suitability to as an answer\n",
        "\n",
        "\n",
        "But here we are not going to check all this details, in this case we just care about the  passage, question and answer because our data set contain a passages and  each passage is following by question and answer.\n",
        "\n",
        "Generally, the question answering system can be\n",
        "classified into two domains:\n",
        "*  **Closed domain question answering system**\n",
        "\n",
        " It deals with questions under a specific\n",
        "domain and can be seen as an easier task because NLP\n",
        "systems can exploit domain-specific knowledge frequently\n",
        "formalized in ontologies.  Alternatively, closed-domain might\n",
        "refer to a situation where only a limited type of questions are\n",
        "accepted, such as questions asking for descriptive rather\n",
        "than procedural information.\n",
        "* ** Open domain question answering system** \n",
        "\n",
        " It is deals with questions about nearly anything, and\n",
        "can only rely on general ontologies and world knowledge.\n",
        "\n",
        "Closed domain question answering system give more exact and correct answer than the open domain question answering system.\n",
        "\n",
        "# Exampe of Q/A:\n",
        "\n",
        "These examples are from the dataset that will use throughout this tutorial :  **Babi tasks**\n",
        "\n",
        "* Task 3 : Three supporting facts . Given a story, the model has to answer a question where only 3 sentences are useful (w.r.t the question)\n",
        "\n",
        "**Task 3: Three Supporting Facts **  \n",
        "John picked up the apple.  \n",
        "John went to the office.  \n",
        "John went to the kitchen.  \n",
        "John dropped the apple.   \n",
        "Where was the apple before the kitchen?  \n",
        "**Answer: office**\n",
        "\n",
        "* Task 2 : Two supporting facts . Given a story, the model has to answer a question where only 2 sentences are useful (w.r.t the question)\n",
        "\n",
        "**Task 2: Two Supporting Facts**  \n",
        "John is in the playground.  \n",
        "John picked up the football.  \n",
        "Bob went to the kitchen.  \n",
        "Where is the football?  \n",
        "**Answer: playground**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## References:\n",
        "\n",
        "1. https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=11&cad=rja&uact=8&ved=2ahUKEwit9rWHls_hAhXKz4UKHUvWCscQFjAKegQIAxAB&url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F320978810_An_Overview_of_Question_Answering_System&usg=AOvVaw01Y_S9X-3MXMpyrFg8mVqL\n",
        "2. https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=13&cad=rja&uact=8&ved=2ahUKEwit9rWHls_hAhXKz4UKHUvWCscQFjAMegQIAhAB&url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3825096%2F&usg=AOvVaw04BMj_m3ELsoau6ylXl6Pn\n"
      ]
    },
    {
      "metadata": {
        "id": "U_QU_pewNEAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ParlAI\n",
        "[ParlAI](https://github.com/facebookresearch/ParlAI/blob/master/README.md) (pronounced “par-lay”) is a framework for dialogue AI research, implemented in Python.\n",
        "\n",
        "Its goal is to provide researchers:\n",
        "\n",
        "* a unified framework for sharing, training and testing dialogue models\n",
        "* many popular datasets available all in one place -- from open-domain chitchat to visual question answering.\n",
        "* a wide set of reference models -- from retrieval baselines to Transformers.\n",
        "* seamless integration of Amazon Mechanical Turk for data collection and human evaluation\n",
        "* integration with Facebook Messenger to connect agents with humans in a chat interface\n",
        "\n",
        "Documentation can be found [here](http://www.parl.ai/static/docs/), some of this tutorial is inspired from the ParlAI documentation so feel free to go back and forth between the notebook and the documentation.\n",
        "\n",
        "\n",
        "### Setup the notebook\n",
        "If using google colab, make sure to use TPU runtime by going to ***Runtime > Change runtime type > Hardware accelerator: TPU > Save***"
      ]
    },
    {
      "metadata": {
        "id": "JqNqCH3XNOp-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install ParlAI\n",
        "\n",
        "Start by installing ParlAI from github. The ParlAI folder will be located in the home directory at `~/ParlAI/`.  \n",
        "*Note: In a jupyter notebook, you can run arbitrary bash commands by prefixing them with a question mark, example: `!echo \"Hello World\"`*"
      ]
    },
    {
      "metadata": {
        "id": "S0YKXmMUNe_C",
        "colab_type": "code",
        "outputId": "6271dd7f-3f12-4dd2-94d1-00b4c1f55107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "# Remove `> /dev/null` to see the output of commands\n",
        "!rm -fr ~/ParlAI\n",
        "!git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI  > /dev/null\n",
        "!cd ~/ParlAI; python setup.py develop > /dev/null"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/root/ParlAI'...\n",
            "remote: Enumerating objects: 20982, done.\u001b[K\n",
            "remote: Total 20982 (delta 0), reused 0 (delta 0), pack-reused 20982\u001b[K\n",
            "Receiving objects: 100% (20982/20982), 17.66 MiB | 30.40 MiB/s, done.\n",
            "Resolving deltas: 100% (14598/14598), done.\n",
            "zip_safe flag not set; analyzing archive contents...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A15LLyvRNU65",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Most of the scripts that we will use in ParlAI are located in the ~/ParlAI/examples directory.\n",
        "Let's have a first glance at the scripts available, we will come back to them later:"
      ]
    },
    {
      "metadata": {
        "id": "8xLsQcefNZr8",
        "colab_type": "code",
        "outputId": "d9b1b32e-f89b-4619-af78-df3771aa0ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "!ls ~/ParlAI/examples/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base_train.py\t       eval_model.py\t\t remote.py\n",
            "build_dict.py\t       extract_image_feature.py  seq2seq_train_babi.py\n",
            "build_pytorch_data.py  interactive.py\t\t train_model.py\n",
            "display_data.py        profile_train.py\n",
            "display_model.py       README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8BCBD6vOY6V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Exploring the data"
      ]
    },
    {
      "metadata": {
        "id": "stay4iHqOcVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**First** we need to download the data, we will use the `build_dict.py` as a dummy task to download the data."
      ]
    },
    {
      "metadata": {
        "id": "9c2i-fGaOQOv",
        "colab_type": "code",
        "outputId": "3058a43e-47b2-4e38-c18d-9680db44776e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "cell_type": "code",
      "source": [
        "# Download the data silently\n",
        "!python ~/ParlAI/examples/build_dict.py --task babi:task1k:1 --dict-file /tmp/babi1.dict > /dev/null\n",
        "# Print a few examples\n",
        "!head -n 30 ~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading babi.tar.gz: 100% 19.2M/19.2M [00:01<00:00, 13.2MB/s]\n",
            "Building dictionary: 100% 900/900 [00:00<00:00, 20.4kex/s]\n",
            "1 Mary moved to the bathroom.\n",
            "2 John went to the hallway.\n",
            "3 Where is Mary? \tbathroom\n",
            "4 Daniel went back to the hallway.\n",
            "5 Sandra moved to the garden.\n",
            "6 Where is Daniel? \thallway\n",
            "7 John moved to the office.\n",
            "8 Sandra journeyed to the bathroom.\n",
            "9 Where is Daniel? \thallway\n",
            "10 Mary moved to the hallway.\n",
            "11 Daniel travelled to the office.\n",
            "12 Where is Daniel? \toffice\n",
            "13 John went back to the garden.\n",
            "14 John moved to the bedroom.\n",
            "15 Where is Sandra? \tbathroom\n",
            "1 Mary went to the bedroom.\n",
            "2 John journeyed to the bathroom.\n",
            "3 Where is John? \tbathroom\n",
            "4 Sandra journeyed to the hallway.\n",
            "5 John journeyed to the garden.\n",
            "6 Where is Mary? \tbedroom\n",
            "7 John journeyed to the bathroom.\n",
            "8 Sandra journeyed to the garden.\n",
            "9 Where is John? \tbathroom\n",
            "10 Sandra went back to the bedroom.\n",
            "11 Daniel travelled to the bathroom.\n",
            "12 Where is John? \tbathroom\n",
            "13 John went to the office.\n",
            "14 Mary moved to the office.\n",
            "15 Where is Sandra? \tbedroom\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "apxdE1h0OoKs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The bAbI tasks were downloaded in `~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/`\n",
        "\n",
        "In bAbI the data is organised as follows:\n",
        "- **Dialog turn**: A dialog turn is a single utterance / statement. Each line in the file corresponds to one dialog turn.   \n",
        "  Example: *\"John went to the office.\"*\n",
        "- **Sample (question)**: Every few dialog turns, a question can be asked that the model has to answer, this consitute a sample.  The question is followed by its ground truth answer, separated by a tab.\n",
        "  Example: *\"Where is John? `<tab>` bathroom\"*\n",
        "- **Episode**: a sequence of ordered coherent dialog turns that are related to each other form an episode. Each new episode is independant of the others. Each line starts with the dialog turn number in the current episode.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0-XeHnQIO_3c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Baseline"
      ]
    },
    {
      "metadata": {
        "id": "zgge2s_2O9z0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now have a clearer idea of the data distribution and the metrics that we can use.  \n",
        "The next step is to start solving the tasks with a simple baseline. This will allow us to compare more elaborate models agains this baseline.  \n",
        "Here are a few classical baselines:\n",
        "- **Random model**: The model answers randomly among the set of possible answers for each question\n",
        "-  **Majority class**: The model always answers with the most frequent answer in the training set (majority class)\n",
        "\n",
        "We are going to reimplement these own baselines.  \n",
        "Implementing a new model in ParlAI is detailed in the [tutorial](http://parl.ai/static/docs/seq2seq_tutorial.html) but for our simple baselines, we will only need to inherit the [Agent](https://github.com/facebookresearch/ParlAI/blob/6d246842d3f4e941dd3806f3d9fa62f607d48f59/parlai/core/agents.py#L50) class and override the `act()` method.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*Note: the `%%writefile` magic command in jupyter writes the content of the cell to a file at the given path.*"
      ]
    },
    {
      "metadata": {
        "id": "zJJQF0MwOwW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/ParlAI/parlai/agents/baseline/  ## create a folder to store our baseline files\n",
        "!touch ~/ParlAI/parlai/agents/baseline/random.py ## create the random baseline file\n",
        "!touch ~/ParlAI/parlai/agents/baseline/majorityclass.py ## create the majority baseline file\n",
        "\n",
        "!mkdir -p ~/ParlAI/parlai/agents/rnn_model/    ## create a folder to store rnn model\n",
        "!touch ~/ParlAI/parlai/agents/rnn_model/rnn_model.py ## create rnn model file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aT1k7cmVQvO7",
        "colab_type": "code",
        "outputId": "bb077c8b-1901-4cf4-fa23-265ace0e7ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile ~/ParlAI/parlai/agents/baseline/random.py\n",
        "import random\n",
        "\n",
        "from parlai.core.torch_agent import Agent   ## import the Agent model where our baseline will inherit\n",
        "\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "  \n",
        "    def act(self):\n",
        "        if 'label_candidates' not in self.observation:   ## check if the attribute label_candidates is part of oservation if not we will not do anything\n",
        "            return\n",
        "        candidates = list(self.observation['label_candidates'])  ## the candidates answer\n",
        "        reply = {'text': candidates[random.randrange(len(candidates))]} ## choose randomly one of the candidate answer\n",
        "        return reply  ## and return the choosen answer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /root/ParlAI/parlai/agents/baseline/random.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VfGjpkISRCRC",
        "colab_type": "code",
        "outputId": "a89ddc63-d1e7-4b00-9526-d01ad5fef8e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/random | grep accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'exs': 1000, 'accuracy': 0.162, 'f1': 0.162, 'bleu': 1.62e-10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k7PfK0lJRHIT",
        "colab_type": "code",
        "outputId": "3bb556aa-5502-43ed-ae18-9f84b5d4c1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1745
        }
      },
      "cell_type": "code",
      "source": [
        "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/random -n 10 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_ignore_fields:  ]\n",
            "[  num_examples: 10 ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: valid ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: baseline/random ]\n",
            "[  model_file: None ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
            "[babi:task10k:1]: Sandra travelled to the office.\n",
            "Sandra went to the bathroom.\n",
            "Where is Sandra?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: bathroom]\n",
            "   kitchen\n",
            "~~\n",
            "[babi:task10k:1]: Mary went to the bedroom.\n",
            "Daniel moved to the hallway.\n",
            "Where is Sandra?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: bathroom]\n",
            "   garden\n",
            "~~\n",
            "[babi:task10k:1]: John went to the garden.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: bathroom]\n",
            "   garden\n",
            "~~\n",
            "[babi:task10k:1]: Daniel journeyed to the bedroom.\n",
            "Daniel travelled to the hallway.\n",
            "Where is John?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   kitchen\n",
            "~~\n",
            "[babi:task10k:1]: John went to the bedroom.\n",
            "John travelled to the office.\n",
            "Where is Daniel?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: hallway]\n",
            "   bathroom\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "~~\n",
            "[babi:task10k:1]: Sandra went back to the bathroom.\n",
            "Mary moved to the garden.\n",
            "Where is Mary?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: garden]\n",
            "   bedroom\n",
            "~~\n",
            "[babi:task10k:1]: Mary went back to the hallway.\n",
            "Sandra went to the office.\n",
            "Where is Sandra?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   bedroom\n",
            "~~\n",
            "[babi:task10k:1]: John went back to the hallway.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   bedroom\n",
            "~~\n",
            "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
            "Daniel moved to the office.\n",
            "Where is John?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   kitchen\n",
            "~~\n",
            "[babi:task10k:1]: Mary went to the office.\n",
            "Sandra went to the office.\n",
            "Where is John?\n",
            "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   garden\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "~~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "485qcDw_RNrx",
        "colab_type": "code",
        "outputId": "51067f46-023d-4ce3-e020-943bd2455d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile ~/ParlAI/parlai/agents/baseline/majorityclass.py\n",
        "import random\n",
        "\n",
        "from parlai.core.torch_agent import Agent\n",
        "\n",
        "\n",
        "class MajorityclassAgent(Agent):\n",
        "  \n",
        "    def act(self):\n",
        "        # From the previous answers:\n",
        "        # Possible answers: Counter({'bathroom': 1564, 'hallway': 1517, 'garden': 1508, 'bedroom': 1473, 'kitchen': 1471, 'office': 1467}) (6)\n",
        "        # So the most common answer is 'bathroom'\n",
        "        reply = {'text': 'bathroom'}\n",
        "        return reply"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /root/ParlAI/parlai/agents/baseline/majorityclass.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "act6OmTGRZEm",
        "colab_type": "code",
        "outputId": "36250739-79c1-4f3f-dc80-f4bd63a8b6cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/majorityclass | grep accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'exs': 1000, 'accuracy': 0.169, 'f1': 0.169, 'bleu': 1.69e-10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "klWsVvXIRaub",
        "colab_type": "code",
        "outputId": "13307f9a-5a78-4407-a2fe-0843180aa54f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1745
        }
      },
      "cell_type": "code",
      "source": [
        "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/majorityclass -n 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_ignore_fields:  ]\n",
            "[  num_examples: 10 ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: valid ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: baseline/majorityclass ]\n",
            "[  model_file: None ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
            "[babi:task10k:1]: Sandra travelled to the office.\n",
            "Sandra went to the bathroom.\n",
            "Where is Sandra?\n",
            "[eval_labels: bathroom]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "~~\n",
            "[babi:task10k:1]: Mary went to the bedroom.\n",
            "Daniel moved to the hallway.\n",
            "Where is Sandra?\n",
            "[eval_labels: bathroom]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "~~\n",
            "[babi:task10k:1]: John went to the garden.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "[eval_labels: bathroom]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "~~\n",
            "[babi:task10k:1]: Daniel journeyed to the bedroom.\n",
            "Daniel travelled to the hallway.\n",
            "Where is John?\n",
            "[eval_labels: office]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "~~\n",
            "[babi:task10k:1]: John went to the bedroom.\n",
            "John travelled to the office.\n",
            "Where is Daniel?\n",
            "[eval_labels: hallway]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "~~\n",
            "[babi:task10k:1]: Sandra went back to the bathroom.\n",
            "Mary moved to the garden.\n",
            "Where is Mary?\n",
            "[eval_labels: garden]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "~~\n",
            "[babi:task10k:1]: Mary went back to the hallway.\n",
            "Sandra went to the office.\n",
            "Where is Sandra?\n",
            "[eval_labels: office]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "~~\n",
            "[babi:task10k:1]: John went back to the hallway.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "[eval_labels: office]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "~~\n",
            "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
            "Daniel moved to the office.\n",
            "Where is John?\n",
            "[eval_labels: office]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "~~\n",
            "[babi:task10k:1]: Mary went to the office.\n",
            "Sandra went to the office.\n",
            "Where is John?\n",
            "[eval_labels: office]\n",
            "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
            "   bathroom\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "~~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cbJkZ5gR0IzD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 : Our RNN Model"
      ]
    },
    {
      "metadata": {
        "id": "73ZNU0_a0VEW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now it is time to create and train our very own model for Question Answering !\n",
        "\n",
        "In order to to so, we will create a class called **RnnModelAgent** that will represent our model in ParlAI and will use a GRU to make the predictions.\n",
        "\n",
        "Our GRU model is made of the following:\n",
        "\n",
        "1) An **Embedding layer** : it is simply a lookup table that given a token index (for example 24 for the word \"bathroom\") will return the corresponding word vector of dimension *hidden_size*. The number of rows in this layer will be equal to the number of words in our dictionnary (since for each of them it has to provide a dense representation : the word  vector)\n",
        "\n",
        "2) A **GRU layer** as explained in Section 0\n",
        "\n",
        "3) A **Decoder** that will transform the output of the GRU into a vector of score for each word of the dictionnary. The higher the score for a word, the most likely this word is the correct answer to the question (according to the model)\n",
        "\n",
        "A ParlAI agent has to implement 3 main methods:\n",
        "\n",
        "1) **__init__**  where we will initialize our model, define useful constants and create the optimizer\n",
        "\n",
        "2) **train_step** : it is the core of our agent. Given a batch of input (stories, questions and answers), we will write the code that compute the model's prediction, the loss of this prediction and apply the backpropagation algorithm on the loss using the optimizer\n",
        "\n",
        "3) **eval_step** : Given a batch of stories and questions (notice that answers are not provided since it is the *evaluation* phase), we will write the code that will output the model's predictions"
      ]
    },
    {
      "metadata": {
        "id": "I-0gkt_TRcDl",
        "colab_type": "code",
        "outputId": "f171ce44-0d03-45aa-fdf0-0a9630f4c148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile ~/ParlAI/parlai/agents/rnn_model/rnn_model.py\n",
        "##import parlai and pytorch\n",
        "from parlai.core.torch_agent import TorchAgent, Output\n",
        "from parlai.core.utils import padded_3d\n",
        "from parlai.core.logs import TensorboardLogger\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "losses = []\n",
        "##define the model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size ## input of the model: number of characters in our dictionary\n",
        "        self.hidden_size = hidden_size ## number of neurons per layers\n",
        "        self.output_size = output_size ## output size of the model: number of charaters in the output\n",
        "        self.n_layers = n_layers  ## number of layers of the model\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)## map a characters to a fixed size of vector (for characters embeddings)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers) ## define the GRU part:it takes asinput the embedding vectors from the encoder\n",
        "        self.decoder = nn.Linear(hidden_size, output_size) ## decode the vectors output of the GRU to characters \n",
        "    \n",
        "    def forward(self, input, hidden): \n",
        "        input = self.encoder(input)\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "        output = self.decoder(output)\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=32):\n",
        "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)) ##initialize all the hidden state to zero\n",
        "\n",
        "class RnnModelAgent(TorchAgent):\n",
        "    \n",
        "    @staticmethod    \n",
        "    def add_cmdline_args(argparser):  ## to allow to run the model on command line\n",
        "        agent = argparser.add_argument_group(\"Simple RNN Arguments\")\n",
        "        \n",
        "        agent.add_argument(\"-hs\", \"--hidden-size\", type=int, default=128, help=\"Size of the hidden layer(s) of the RNN\")\n",
        "        agent.add_argument(\"-nl\", \"--num-layers\", type=int, default=1, help=\"Number of layers of the RNN\")\n",
        "        \n",
        "        TorchAgent.add_cmdline_args(argparser)\n",
        "        RnnModelAgent.dictionary_class().add_cmdline_args(argparser)\n",
        "        \n",
        "        return agent\n",
        "    \n",
        "    def __init__(self, opt, shared=None):\n",
        "        \n",
        "        super().__init__(opt, shared)\n",
        "        \n",
        "        if opt['tensorboard_log'] is True:\n",
        "            self.writer = TensorboardLogger(opt)\n",
        "        \n",
        "        self.dictionnary_size = 26      ## we suppose that our dictionary contain the 26 characters of the alphabet\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
        "        self.use_cuda = True if torch.cuda.is_available() else False\n",
        "        self.embedding_dim = opt[\"hidden_size\"]  ##the dimension of our embemding vector after transforming the character to a vector\n",
        "        self.n_layers = opt[\"num_layers\"]  ## number of layer of the network\n",
        "        self.batch_size = opt[\"batchsize\"]  ## number of examples that we are going to pass as an input at the same time to the network\n",
        "        self.criterion = nn.NLLLoss()  ### define the loss of our network\n",
        "        print(\"Init called!\")\n",
        "          \n",
        "        if (shared is not None) and (\"decoder\" in shared):\n",
        "          self.decoder = shared[\"decoder\"]\n",
        "          print(\"Shared\")\n",
        "          \n",
        "        else:\n",
        "          self.decoder = RNN(self.dictionnary_size, self.embedding_dim, self.dictionnary_size, self.n_layers) ## This RNN refer to the previous RNN class\n",
        "          print(\"Not shared\")\n",
        "          \n",
        "          \n",
        "        self.decoder.to(self.device)\n",
        "        self.model = self.decoder\n",
        "        \n",
        "        def weight_init(m):   ##initialisation of the weights of our model using xavier\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "        \n",
        "        self.decoder.apply(weight_init)    #apply the initialized weight to the model\n",
        "        self.optimizer = optim.Adam(self.decoder.parameters()) ## use Adam optimizer for the model to perform backpropagation(weight updating)\n",
        "        self.scheduler = None\n",
        "        self.warmup_scheduler = None\n",
        "        self.batch_iter = 0\n",
        "\n",
        "        \n",
        "        \n",
        "    def vectorize(self, *args, **kwargs):    ## this function is called by parlai to preprocess text\n",
        "        \"\"\"Override options in vectorize from parent.\"\"\"\n",
        "        kwargs['add_start'] = False       ##to avoid starting at  the start of sentence\n",
        "        kwargs['add_end'] = False\n",
        "        return super().vectorize(*args, **kwargs)\n",
        "      \n",
        "      \n",
        "    ## the next function allows us to train the model we defined so far\n",
        "    def train_step(self, batch):  \n",
        "        \n",
        "        self.decoder.train()\n",
        "        \n",
        "        contexts, answers = batch.text_vec, batch.label_vec # extract the different contexts and answers from the batch input\n",
        "        \n",
        "        self.optimizer.zero_grad() ## Since the backward() function accumulates gradients, and we don’t want to mix up gradients between minibatches,\n",
        "                                   ## we have to zero them out at the start of a new minibatch\n",
        "\n",
        "        loss = 0\n",
        "        hidden = self.decoder.init_hidden(self.batch_size).to(self.device) ## initialize the hidden state to zero by calling the previous init_hidden function\n",
        "\n",
        "        output, hidden = self.decoder(contexts.t(), hidden) \n",
        "        output = output.transpose(0,1)[:,-1,:]              ## to get sequence leng, batch, dimension\n",
        "        loss += self.criterion(output.squeeze(1), answers.squeeze(1)) ## compute the loss by comparing the output of our model and the true answer\n",
        "            \n",
        "        \n",
        "        pred = output.argmax(dim=1)  ## get the predicted answer of our model: it appear at the index which has the bigger value of probbility\n",
        "        \n",
        "        losses.append(loss.item())  ## accumulate the loss\n",
        "         \n",
        "        loss.backward() ## backpropagate the loss to update the parameters of the model\n",
        "        self.optimizer.step()  #specify that we finish with one step optimization \n",
        "        \n",
        "        self.batch_iter+= 1 ## next iteration\n",
        "        \n",
        "        return Output(self.dict.vec2txt(pred).split(\" \")) # convert the vector output to text and return it\n",
        "    \n",
        "    def eval_step(self, batch): #after training the model we need to evaluate it in order to measure its performance\n",
        "      \n",
        "        self.decoder.eval()\n",
        "        \n",
        "        contexts = batch.text_vec ## in the evaluation case we don't have acces to an asnwer, the only thing we have is the question\n",
        "        \n",
        "        hidden = self.decoder.init_hidden(contexts.shape[0]).to(self.device) ## as usually we initialize the hidden state\n",
        "\n",
        "        output, hidden = self.decoder(contexts.t(), hidden)  # compute the output of the model on unseen data\n",
        "        output = output.transpose(0,1)[:,-1,:]    ## to get sequence leng, batch, dimension\n",
        "            \n",
        "        \n",
        "        pred = output.argmax(dim=1)  # get the predicte\n",
        "        \n",
        "        #print(\"Batch keys : \", batch.keys())\n",
        "        #print(\"Pred shape : \", pred.shape)\n",
        "        #print(\"Text vec : \", batch.text_vec.shape)\n",
        "        #print(\"Predictions : \", pred.shape)\n",
        "        \n",
        "        #print(self.decoder.gru.weight_hh_l0.data)\n",
        "        \n",
        "        return Output(self.dict.vec2txt(pred).split(\" \"))\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /root/ParlAI/parlai/agents/rnn_model/rnn_model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PJjg2kKiVn-Q",
        "colab_type": "code",
        "outputId": "d753c0cc-39cf-4f72-d267-b8dfef95160b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "#!rm -fr /tmp/*\n",
        "!ls /tmp/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "babi1.dict\t       my_rnn_model.test\tseq2seq_model.opt\n",
            "babi1.dict.opt\t       my_rnn_model.trainstats\tseq2seq_model.test\n",
            "my_rnn_model\t       my_rnn_model.valid\tseq2seq_model.trainstats\n",
            "my_rnn_model.dict      seq2seq_model\t\tseq2seq_model.valid\n",
            "my_rnn_model.dict.opt  seq2seq_model.dict\n",
            "my_rnn_model.opt       seq2seq_model.dict.opt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IiNtn4-Ht8s0",
        "colab_type": "code",
        "outputId": "31151490-43c0-409d-c580-c07f57786105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6263
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -fr /tmp/my_rnn_model*\n",
        "!python ~/ParlAI/examples/train_model.py -t babi:task10k:1 -bs 32 -eps 10 -m rnn_model -mf /tmp/my_rnn_model --dict-file /tmp/babi1.dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 32 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: rnn_model ]\n",
            "[  model_file: /tmp/my_rnn_model ]\n",
            "[ Training Loop Arguments: ] \n",
            "[  dict_build_first: True ]\n",
            "[  display_examples: False ]\n",
            "[  eval_batchsize: None ]\n",
            "[  evaltask: None ]\n",
            "[  load_from_checkpoint: False ]\n",
            "[  max_train_time: -1 ]\n",
            "[  num_epochs: 10.0 ]\n",
            "[  save_after_valid: False ]\n",
            "[  save_every_n_secs: -1 ]\n",
            "[  short_final_eval: False ]\n",
            "[  validation_cutoff: 1.0 ]\n",
            "[  validation_every_n_epochs: -1 ]\n",
            "[  validation_every_n_secs: -1 ]\n",
            "[  validation_max_exs: -1 ]\n",
            "[  validation_metric: accuracy ]\n",
            "[  validation_metric_mode: None ]\n",
            "[  validation_patience: 10 ]\n",
            "[  validation_share_agent: False ]\n",
            "[ Tensorboard Arguments: ] \n",
            "[  tensorboard_comment:  ]\n",
            "[  tensorboard_log: False ]\n",
            "[  tensorboard_metrics: None ]\n",
            "[  tensorboard_tag: None ]\n",
            "[ PytorchData Arguments: ] \n",
            "[  batch_length_range: 5 ]\n",
            "[  batch_sort_cache_type: pop ]\n",
            "[  batch_sort_field: text ]\n",
            "[  numworkers: 4 ]\n",
            "[  pytorch_context_length: -1 ]\n",
            "[  pytorch_datapath: None ]\n",
            "[  pytorch_include_labels: True ]\n",
            "[  pytorch_preprocess: False ]\n",
            "[  pytorch_teacher_batch_sort: False ]\n",
            "[  pytorch_teacher_dataset: None ]\n",
            "[  pytorch_teacher_task: None ]\n",
            "[  shuffle: False ]\n",
            "[ Dictionary Loop Arguments: ] \n",
            "[  dict_include_test: False ]\n",
            "[  dict_include_valid: False ]\n",
            "[  dict_maxexs: -1 ]\n",
            "[  log_every_n_secs: 2 ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Simple RNN Arguments: ] \n",
            "[  hidden_size: 128 ]\n",
            "[  num_layers: 1 ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  betas: (0.9, 0.999) ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  fp16: False ]\n",
            "[  gpu: -1 ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  history_size: -1 ]\n",
            "[  label_truncate: None ]\n",
            "[  learningrate: 1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  no_cuda: False ]\n",
            "[  nus: (0.7,) ]\n",
            "[  optimizer: sgd ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: -1 ]\n",
            "[  update_freq: -1 ]\n",
            "[  use_reply: label ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: /tmp/babi1.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "\n",
            "********************************************************************************\n",
            "Thank you for using ParlAI! We are conducting a user survey.\n",
            "Please consider filling it out at https://forms.gle/uEFbYGP7w6hiuGQT9\n",
            "********************************************************************************\n",
            "\n",
            "[ building dictionary first... ]\n",
            "[ dictionary already built .]\n",
            "[ no model with opt yet at: /tmp/my_rnn_model(.opt) ]\n",
            "Dictionary: loading dictionary from /tmp/babi1.dict\n",
            "[ num words =  26 ]\n",
            "[ Using CUDA ]\n",
            "Init called!\n",
            "Not shared\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "[ training... ]\n",
            "[ time:2.0s total_exs:5088 epochs:0.57 time_left:34.0s ] {'exs': 5088, 'accuracy': 0.411, 'f1': 0.411, 'bleu': 4.11e-10, 'num_updates': 0}\n",
            "[ time:4.0s total_exs:10272 epochs:1.14 time_left:32.0s ] {'exs': 5184, 'accuracy': 0.5143, 'f1': 0.5143, 'bleu': 5.143e-10, 'num_updates': 0}\n",
            "[ time:6.0s total_exs:15488 epochs:1.72 time_left:29.0s ] {'exs': 5216, 'accuracy': 0.5146, 'f1': 0.5146, 'bleu': 5.146e-10, 'num_updates': 0}\n",
            "[ time:8.0s total_exs:20672 epochs:2.3 time_left:27.0s ] {'exs': 5184, 'accuracy': 0.5235, 'f1': 0.5235, 'bleu': 5.235e-10, 'num_updates': 0}\n",
            "[ time:10.0s total_exs:25728 epochs:2.86 time_left:26.0s ] {'exs': 5056, 'accuracy': 0.5099, 'f1': 0.5099, 'bleu': 5.099e-10, 'num_updates': 0}\n",
            "[ time:12.0s total_exs:30400 epochs:3.38 time_left:24.0s ] {'exs': 4672, 'accuracy': 0.5002, 'f1': 0.5002, 'bleu': 5.002e-10, 'num_updates': 0}\n",
            "[ time:14.0s total_exs:35040 epochs:3.89 time_left:23.0s ] {'exs': 4640, 'accuracy': 0.5181, 'f1': 0.5181, 'bleu': 5.181e-10, 'num_updates': 0}\n",
            "[ time:16.0s total_exs:39648 epochs:4.41 time_left:21.0s ] {'exs': 4608, 'accuracy': 0.5226, 'f1': 0.5226, 'bleu': 5.226e-10, 'num_updates': 0}\n",
            "[ time:18.0s total_exs:44288 epochs:4.92 time_left:19.0s ] {'exs': 4640, 'accuracy': 0.522, 'f1': 0.522, 'bleu': 5.22e-10, 'num_updates': 0}\n",
            "[ time:20.0s total_exs:49056 epochs:5.45 time_left:17.0s ] {'exs': 4768, 'accuracy': 0.5017, 'f1': 0.5017, 'bleu': 5.017e-10, 'num_updates': 0}\n",
            "[ time:22.0s total_exs:54208 epochs:6.02 time_left:15.0s ] {'exs': 5152, 'accuracy': 0.5262, 'f1': 0.5262, 'bleu': 5.262e-10, 'num_updates': 0}\n",
            "[ time:24.0s total_exs:59424 epochs:6.6 time_left:13.0s ] {'exs': 5216, 'accuracy': 0.5247, 'f1': 0.5247, 'bleu': 5.247e-10, 'num_updates': 0}\n",
            "[ time:26.0s total_exs:64640 epochs:7.18 time_left:11.0s ] {'exs': 5216, 'accuracy': 0.5334, 'f1': 0.5334, 'bleu': 5.334e-10, 'num_updates': 0}\n",
            "[ time:28.0s total_exs:69856 epochs:7.76 time_left:9.0s ] {'exs': 5216, 'accuracy': 0.5464, 'f1': 0.5464, 'bleu': 5.464e-10, 'num_updates': 0}\n",
            "[ time:30.0s total_exs:75040 epochs:8.34 time_left:7.0s ] {'exs': 5184, 'accuracy': 0.553, 'f1': 0.553, 'bleu': 5.53e-10, 'num_updates': 0}\n",
            "[ time:32.0s total_exs:80224 epochs:8.91 time_left:4.0s ] {'exs': 5184, 'accuracy': 0.5673, 'f1': 0.5673, 'bleu': 5.673e-10, 'num_updates': 0}\n",
            "[ time:34.0s total_exs:85344 epochs:9.48 time_left:2.0s ] {'exs': 5120, 'accuracy': 0.5824, 'f1': 0.5824, 'bleu': 5.824e-10, 'num_updates': 0}\n",
            "[ time:36.0s total_exs:89920 epochs:9.99 time_left:1.0s ] {'exs': 4576, 'accuracy': 0.606, 'f1': 0.606, 'bleu': 6.06e-10, 'num_updates': 0}\n",
            "[ time:36.0s total_exs:90016 epochs:10.0 time_left:0s ] {'exs': 96, 'accuracy': 0.6979, 'f1': 0.6979, 'bleu': 6.979e-10, 'num_updates': 0}\n",
            "[ num_epochs completed:10.0 time elapsed:36.15867471694946s ]\n",
            "Dictionary: saving dictionary to /tmp/my_rnn_model.dict\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "[ running eval: valid ]\n",
            "valid:{'exs': 1000, 'accuracy': 0.55, 'f1': 0.55, 'bleu': 5.5e-10, 'num_updates': 0}\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_test.txt]\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "Init called!\n",
            "Not shared\n",
            "[ running eval: test ]\n",
            "test:{'exs': 1000, 'accuracy': 0.504, 'f1': 0.504, 'bleu': 5.04e-10, 'num_updates': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c4NmxeDbYDay",
        "colab_type": "code",
        "outputId": "8db57355-187b-41d0-9228-bc3b77fce137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1835
        }
      },
      "cell_type": "code",
      "source": [
        "!python ~/ParlAI/examples/eval_model.py -mf /tmp/my_rnn_model -t babi:task10k:1 -dt valid --dict-file /tmp/babi1.dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['datatype'] to valid (previously: train )]\n",
            "Dictionary: loading dictionary from /tmp/babi1.dict\n",
            "[ num words =  26 ]\n",
            "[ Using CUDA ]\n",
            "Init called!\n",
            "Not shared\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  log_every_n_secs: 2 ]\n",
            "[  metrics: all ]\n",
            "[  num_examples: -1 ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 32 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: valid ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: rnn_model ]\n",
            "[  model_file: /tmp/my_rnn_model ]\n",
            "[ PytorchData Arguments: ] \n",
            "[  batch_length_range: 5 ]\n",
            "[  batch_sort_cache_type: pop ]\n",
            "[  batch_sort_field: text ]\n",
            "[  numworkers: 4 ]\n",
            "[  pytorch_context_length: -1 ]\n",
            "[  pytorch_datapath: None ]\n",
            "[  pytorch_include_labels: True ]\n",
            "[  pytorch_preprocess: False ]\n",
            "[  pytorch_teacher_batch_sort: False ]\n",
            "[  pytorch_teacher_dataset: None ]\n",
            "[  pytorch_teacher_task: None ]\n",
            "[  shuffle: False ]\n",
            "[ Tensorboard Arguments: ] \n",
            "[  tensorboard_comment:  ]\n",
            "[  tensorboard_log: False ]\n",
            "[  tensorboard_metrics: None ]\n",
            "[  tensorboard_tag: None ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Simple RNN Arguments: ] \n",
            "[  hidden_size: 128 ]\n",
            "[  num_layers: 1 ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  betas: [0.9, 0.999] ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  fp16: False ]\n",
            "[  gpu: -1 ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  history_size: -1 ]\n",
            "[  label_truncate: None ]\n",
            "[  learningrate: 1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  no_cuda: False ]\n",
            "[  nus: [0.7] ]\n",
            "[  optimizer: sgd ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: -1 ]\n",
            "[  update_freq: -1 ]\n",
            "[  use_reply: label ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: /tmp/babi1.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "EPOCH DONE\n",
            "finished evaluating task babi:task10k:1 using datatype valid\n",
            "{'exs': 1000, 'accuracy': 0, 'f1': 0, 'bleu': 0, 'num_updates': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8rzVhJs74k0W",
        "colab_type": "code",
        "outputId": "c9c108b7-51ca-4341-d2fe-4c1bb371c56c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3203
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -fr /tmp/seq2seq_model*\n",
        "!python ~/ParlAI/examples/train_model.py -t babi:task10k:1 -bs 32 -eps 10 -m seq2seq -mf /tmp/seq2seq_model --dict-file /tmp/babi1.dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 32 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: seq2seq ]\n",
            "[  model_file: /tmp/seq2seq_model ]\n",
            "[ Training Loop Arguments: ] \n",
            "[  dict_build_first: True ]\n",
            "[  display_examples: False ]\n",
            "[  eval_batchsize: None ]\n",
            "[  evaltask: None ]\n",
            "[  load_from_checkpoint: False ]\n",
            "[  max_train_time: -1 ]\n",
            "[  num_epochs: 10.0 ]\n",
            "[  save_after_valid: False ]\n",
            "[  save_every_n_secs: -1 ]\n",
            "[  short_final_eval: False ]\n",
            "[  validation_cutoff: 1.0 ]\n",
            "[  validation_every_n_epochs: -1 ]\n",
            "[  validation_every_n_secs: -1 ]\n",
            "[  validation_max_exs: -1 ]\n",
            "[  validation_metric: accuracy ]\n",
            "[  validation_metric_mode: None ]\n",
            "[  validation_patience: 10 ]\n",
            "[  validation_share_agent: False ]\n",
            "[ Tensorboard Arguments: ] \n",
            "[  tensorboard_comment:  ]\n",
            "[  tensorboard_log: False ]\n",
            "[  tensorboard_metrics: None ]\n",
            "[  tensorboard_tag: None ]\n",
            "[ PytorchData Arguments: ] \n",
            "[  batch_length_range: 5 ]\n",
            "[  batch_sort_cache_type: pop ]\n",
            "[  batch_sort_field: text ]\n",
            "[  numworkers: 4 ]\n",
            "[  pytorch_context_length: -1 ]\n",
            "[  pytorch_datapath: None ]\n",
            "[  pytorch_include_labels: True ]\n",
            "[  pytorch_preprocess: False ]\n",
            "[  pytorch_teacher_batch_sort: False ]\n",
            "[  pytorch_teacher_dataset: None ]\n",
            "[  pytorch_teacher_task: None ]\n",
            "[  shuffle: False ]\n",
            "[ Dictionary Loop Arguments: ] \n",
            "[  dict_include_test: False ]\n",
            "[  dict_include_valid: False ]\n",
            "[  dict_maxexs: -1 ]\n",
            "[  log_every_n_secs: 2 ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Seq2Seq Arguments: ] \n",
            "[  attention: none ]\n",
            "[  attention_length: 48 ]\n",
            "[  attention_time: post ]\n",
            "[  bidirectional: False ]\n",
            "[  decoder: same ]\n",
            "[  dropout: 0.1 ]\n",
            "[  embeddingsize: 128 ]\n",
            "[  hiddensize: 128 ]\n",
            "[  input_dropout: 0.0 ]\n",
            "[  lookuptable: unique ]\n",
            "[  numlayers: 2 ]\n",
            "[  numsoftmax: 1 ]\n",
            "[  rnn_class: lstm ]\n",
            "[ Torch Generator Agent: ] \n",
            "[  beam_block_ngram: 0 ]\n",
            "[  beam_dot_log: False ]\n",
            "[  beam_min_length: 1 ]\n",
            "[  beam_min_n_best: 3 ]\n",
            "[  beam_size: 1 ]\n",
            "[  skip_generation: False ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  betas: (0.9, 0.999) ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  fp16: False ]\n",
            "[  gpu: -1 ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  history_size: -1 ]\n",
            "[  label_truncate: None ]\n",
            "[  learningrate: 1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  no_cuda: False ]\n",
            "[  nus: (0.7,) ]\n",
            "[  optimizer: sgd ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: -1 ]\n",
            "[  update_freq: -1 ]\n",
            "[  use_reply: label ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: /tmp/babi1.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "\n",
            "********************************************************************************\n",
            "Thank you for using ParlAI! We are conducting a user survey.\n",
            "Please consider filling it out at https://forms.gle/uEFbYGP7w6hiuGQT9\n",
            "********************************************************************************\n",
            "\n",
            "[ building dictionary first... ]\n",
            "[ dictionary already built .]\n",
            "[ no model with opt yet at: /tmp/seq2seq_model(.opt) ]\n",
            "Dictionary: loading dictionary from /tmp/babi1.dict\n",
            "[ num words =  26 ]\n",
            "[ Using CUDA ]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
            "[ training... ]\n",
            "[ time:2.0s total_exs:3680 epochs:0.41 time_left:48.0s ] {'exs': 3680, 'lr': 1, 'num_updates': 115, 'loss': 126.1, 'token_acc': 0.5721, 'nll_loss': 1.116, 'ppl': 3.052}\n",
            "[ time:4.0s total_exs:7680 epochs:0.85 time_left:44.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 240, 'loss': 113.1, 'token_acc': 0.6308, 'nll_loss': 0.9045, 'ppl': 2.471}\n",
            "[ time:6.0s total_exs:11680 epochs:1.3 time_left:41.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 365, 'loss': 99.38, 'token_acc': 0.725, 'nll_loss': 0.795, 'ppl': 2.215}\n",
            "[ time:8.0s total_exs:15680 epochs:1.74 time_left:39.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 490, 'loss': 92.47, 'token_acc': 0.7586, 'nll_loss': 0.7398, 'ppl': 2.095}\n",
            "[ time:10.0s total_exs:19680 epochs:2.19 time_left:36.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 615, 'loss': 88.58, 'token_acc': 0.764, 'nll_loss': 0.7086, 'ppl': 2.031}\n",
            "[ time:12.0s total_exs:23680 epochs:2.63 time_left:34.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 740, 'loss': 88.15, 'token_acc': 0.7548, 'nll_loss': 0.7052, 'ppl': 2.024}\n",
            "[ time:14.0s total_exs:27680 epochs:3.08 time_left:32.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 865, 'loss': 85.36, 'token_acc': 0.7426, 'nll_loss': 0.6829, 'ppl': 1.98}\n",
            "[ time:16.0s total_exs:31712 epochs:3.52 time_left:30.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 991, 'loss': 79.51, 'token_acc': 0.7555, 'nll_loss': 0.631, 'ppl': 1.88}\n",
            "[ time:18.0s total_exs:35744 epochs:3.97 time_left:28.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1117, 'loss': 79.46, 'token_acc': 0.7555, 'nll_loss': 0.6306, 'ppl': 1.879}\n",
            "[ time:20.0s total_exs:39776 epochs:4.42 time_left:26.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1243, 'loss': 78.56, 'token_acc': 0.7464, 'nll_loss': 0.6235, 'ppl': 1.865}\n",
            "[ time:22.0s total_exs:43808 epochs:4.87 time_left:24.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1369, 'loss': 77.57, 'token_acc': 0.7471, 'nll_loss': 0.6156, 'ppl': 1.851}\n",
            "[ time:24.0s total_exs:47808 epochs:5.31 time_left:22.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 1494, 'loss': 77.45, 'token_acc': 0.7525, 'nll_loss': 0.6196, 'ppl': 1.858}\n",
            "[ time:26.0s total_exs:51840 epochs:5.76 time_left:20.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1620, 'loss': 77.44, 'token_acc': 0.7458, 'nll_loss': 0.6146, 'ppl': 1.849}\n",
            "[ time:28.0s total_exs:55808 epochs:6.2 time_left:18.0s ] {'exs': 3968, 'lr': 1, 'num_updates': 1744, 'loss': 74.97, 'token_acc': 0.749, 'nll_loss': 0.6046, 'ppl': 1.831}\n",
            "[ time:30.0s total_exs:59808 epochs:6.65 time_left:16.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 1869, 'loss': 72.98, 'token_acc': 0.757, 'nll_loss': 0.5838, 'ppl': 1.793}\n",
            "[ time:32.0s total_exs:63808 epochs:7.09 time_left:14.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 1994, 'loss': 76.11, 'token_acc': 0.7479, 'nll_loss': 0.6089, 'ppl': 1.838}\n",
            "[ time:34.0s total_exs:67840 epochs:7.54 time_left:12.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 2120, 'loss': 74.52, 'token_acc': 0.7491, 'nll_loss': 0.5914, 'ppl': 1.807}\n",
            "[ time:36.0s total_exs:71840 epochs:7.98 time_left:10.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 2245, 'loss': 73.42, 'token_acc': 0.751, 'nll_loss': 0.5873, 'ppl': 1.799}\n",
            "[ time:38.0s total_exs:75840 epochs:8.43 time_left:8.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 2370, 'loss': 73.16, 'token_acc': 0.7506, 'nll_loss': 0.5853, 'ppl': 1.796}\n",
            "[ time:40.0s total_exs:79872 epochs:8.87 time_left:6.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 2496, 'loss': 72.32, 'token_acc': 0.7536, 'nll_loss': 0.574, 'ppl': 1.775}\n",
            "[ time:42.0s total_exs:83872 epochs:9.32 time_left:4.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 2621, 'loss': 72.37, 'token_acc': 0.7576, 'nll_loss': 0.579, 'ppl': 1.784}\n",
            "[ time:44.0s total_exs:87904 epochs:9.77 time_left:2.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 2747, 'loss': 70.87, 'token_acc': 0.755, 'nll_loss': 0.5625, 'ppl': 1.755}\n",
            "[ time:45.0s total_exs:90016 epochs:10.0 time_left:0s ] {'exs': 2112, 'lr': 1, 'num_updates': 2813, 'loss': 37.5, 'token_acc': 0.7543, 'nll_loss': 0.5682, 'ppl': 1.765}\n",
            "[ num_epochs completed:10.0 time elapsed:45.2425332069397s ]\n",
            "Dictionary: saving dictionary to /tmp/seq2seq_model.dict\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
            "[ running eval: valid ]\n",
            "valid:{'exs': 1000, 'accuracy': 0.54, 'f1': 0.54, 'bleu': 5.4e-10, 'lr': 1, 'num_updates': 2813, 'loss': 18.56, 'token_acc': 0.77, 'nll_loss': 0.5234, 'ppl': 1.688}\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_test.txt]\n",
            "[ running eval: test ]\n",
            "test:{'exs': 1000, 'accuracy': 0.507, 'f1': 0.507, 'bleu': 5.07e-10, 'lr': 1, 'num_updates': 2813, 'loss': 19.91, 'token_acc': 0.7535, 'nll_loss': 0.5744, 'ppl': 1.776}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WTJbs-xV49X-",
        "colab_type": "code",
        "outputId": "654dde76-5ff5-4729-80d3-4c4f0d140e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2177
        }
      },
      "cell_type": "code",
      "source": [
        "!python ~/ParlAI/examples/eval_model.py -mf /tmp/seq2seq_model -t babi:task10k:1 -dt valid -bs 32 --dict-file /tmp/babi1.dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['datatype'] to valid (previously: train )]\n",
            "Dictionary: loading dictionary from /tmp/seq2seq_model.dict\n",
            "[ num words =  26 ]\n",
            "[ Using CUDA ]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "[ Loading existing model params from /tmp/seq2seq_model ]\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  log_every_n_secs: 2 ]\n",
            "[  metrics: all ]\n",
            "[  num_examples: -1 ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 32 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: valid ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: seq2seq ]\n",
            "[  model_file: /tmp/seq2seq_model ]\n",
            "[ PytorchData Arguments: ] \n",
            "[  batch_length_range: 5 ]\n",
            "[  batch_sort_cache_type: pop ]\n",
            "[  batch_sort_field: text ]\n",
            "[  numworkers: 4 ]\n",
            "[  pytorch_context_length: -1 ]\n",
            "[  pytorch_datapath: None ]\n",
            "[  pytorch_include_labels: True ]\n",
            "[  pytorch_preprocess: False ]\n",
            "[  pytorch_teacher_batch_sort: False ]\n",
            "[  pytorch_teacher_dataset: None ]\n",
            "[  pytorch_teacher_task: None ]\n",
            "[  shuffle: False ]\n",
            "[ Tensorboard Arguments: ] \n",
            "[  tensorboard_comment:  ]\n",
            "[  tensorboard_log: False ]\n",
            "[  tensorboard_metrics: None ]\n",
            "[  tensorboard_tag: None ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Seq2Seq Arguments: ] \n",
            "[  attention: none ]\n",
            "[  attention_length: 48 ]\n",
            "[  attention_time: post ]\n",
            "[  bidirectional: False ]\n",
            "[  decoder: same ]\n",
            "[  dropout: 0.1 ]\n",
            "[  embeddingsize: 128 ]\n",
            "[  hiddensize: 128 ]\n",
            "[  input_dropout: 0.0 ]\n",
            "[  lookuptable: unique ]\n",
            "[  numlayers: 2 ]\n",
            "[  numsoftmax: 1 ]\n",
            "[  rnn_class: lstm ]\n",
            "[ Torch Generator Agent: ] \n",
            "[  beam_block_ngram: 0 ]\n",
            "[  beam_dot_log: False ]\n",
            "[  beam_min_length: 1 ]\n",
            "[  beam_min_n_best: 3 ]\n",
            "[  beam_size: 1 ]\n",
            "[  skip_generation: False ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  betas: [0.9, 0.999] ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  fp16: False ]\n",
            "[  gpu: -1 ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  history_size: -1 ]\n",
            "[  label_truncate: None ]\n",
            "[  learningrate: 1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  no_cuda: False ]\n",
            "[  nus: [0.7] ]\n",
            "[  optimizer: sgd ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: -1 ]\n",
            "[  update_freq: -1 ]\n",
            "[  use_reply: label ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: /tmp/seq2seq_model.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "EPOCH DONE\n",
            "finished evaluating task babi:task10k:1 using datatype valid\n",
            "{'exs': 1000, 'accuracy': 0.54, 'f1': 0.54, 'bleu': 5.4e-10, 'num_updates': 0, 'loss': 18.56, 'token_acc': 0.77, 'nll_loss': 0.5234, 'ppl': 1.688}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bQqmedIT3FBI",
        "colab_type": "code",
        "outputId": "b0983b2a-0b62-4994-d787-f554d86426dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5093
        }
      },
      "cell_type": "code",
      "source": [
        "!cat /root/ParlAI/parlai/agents/example_seq2seq/example_seq2seq.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/usr/bin/env python3\n",
            "\n",
            "# Copyright (c) Facebook, Inc. and its affiliates.\n",
            "# This source code is licensed under the MIT license found in the\n",
            "# LICENSE file in the root directory of this source tree.\n",
            "\"\"\"Example sequence to sequence agent for ParlAI \"Creating an Agent\" tutorial.\n",
            "http://parl.ai/static/docs/tutorial_seq2seq.html\n",
            "\"\"\"\n",
            "\n",
            "from parlai.core.torch_agent import TorchAgent, Output\n",
            "\n",
            "import torch\n",
            "from torch import optim\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    \"\"\"Encodes the input context.\"\"\"\n",
            "\n",
            "    def __init__(self, input_size, hidden_size, numlayers):\n",
            "        \"\"\"Initialize encoder.\n",
            "\n",
            "        :param input_size: size of embedding\n",
            "        :param hidden_size: size of GRU hidden layers\n",
            "        :param numlayers: number of GRU layers\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "\n",
            "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=numlayers,\n",
            "                          batch_first=True)\n",
            "\n",
            "    def forward(self, input, hidden=None):\n",
            "        \"\"\"Return encoded state.\n",
            "\n",
            "        :param input: (batchsize x seqlen) tensor of token indices.\n",
            "        :param hidden: optional past hidden state\n",
            "        \"\"\"\n",
            "        embedded = self.embedding(input)\n",
            "        output, hidden = self.gru(embedded, hidden)\n",
            "        return output, hidden\n",
            "\n",
            "\n",
            "class DecoderRNN(nn.Module):\n",
            "    \"\"\"Generates a sequence of tokens in response to context.\"\"\"\n",
            "\n",
            "    def __init__(self, output_size, hidden_size, numlayers):\n",
            "        \"\"\"Initialize decoder.\n",
            "\n",
            "        :param input_size: size of embedding\n",
            "        :param hidden_size: size of GRU hidden layers\n",
            "        :param numlayers: number of GRU layers\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "\n",
            "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=numlayers,\n",
            "                          batch_first=True)\n",
            "        self.out = nn.Linear(hidden_size, output_size)\n",
            "        self.softmax = nn.LogSoftmax(dim=2)\n",
            "\n",
            "    def forward(self, input, hidden):\n",
            "        \"\"\"Return encoded state.\n",
            "\n",
            "        :param input: batch_size x 1 tensor of token indices.\n",
            "        :param hidden: past (e.g. encoder) hidden state\n",
            "        \"\"\"\n",
            "        emb = self.embedding(input)\n",
            "        rel = F.relu(emb)\n",
            "        output, hidden = self.gru(rel, hidden)\n",
            "        scores = self.softmax(self.out(output))\n",
            "        return scores, hidden\n",
            "\n",
            "\n",
            "class ExampleSeq2seqAgent(TorchAgent):\n",
            "    \"\"\"Agent which takes an input sequence and produces an output sequence.\n",
            "\n",
            "    This model is based on Sean Robertson's `seq2seq tutorial\n",
            "    <http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`_.\n",
            "    \"\"\"\n",
            "\n",
            "    @classmethod\n",
            "    def add_cmdline_args(cls, argparser):\n",
            "        \"\"\"Add command-line arguments specifically for this agent.\"\"\"\n",
            "        super(ExampleSeq2seqAgent, cls).add_cmdline_args(argparser)\n",
            "        agent = argparser.add_argument_group('Seq2Seq Arguments')\n",
            "        agent.add_argument('-hs', '--hiddensize', type=int, default=128,\n",
            "                           help='size of the hidden layers')\n",
            "        agent.add_argument('-esz', '--embeddingsize', type=int, default=128,\n",
            "                           help='size of the token embeddings')\n",
            "        agent.add_argument('-nl', '--numlayers', type=int, default=2,\n",
            "                           help='number of hidden layers')\n",
            "        agent.add_argument('-lr', '--learningrate', type=float, default=1,\n",
            "                           help='learning rate')\n",
            "        agent.add_argument('-dr', '--dropout', type=float, default=0.1,\n",
            "                           help='dropout rate')\n",
            "        agent.add_argument('--gpu', type=int, default=-1,\n",
            "                           help='which GPU device to use')\n",
            "        agent.add_argument('-rf', '--report-freq', type=float, default=0.001,\n",
            "                           help='Report frequency of prediction during eval.')\n",
            "        ExampleSeq2seqAgent.dictionary_class().add_cmdline_args(argparser)\n",
            "        return agent\n",
            "\n",
            "    def __init__(self, opt, shared=None):\n",
            "        \"\"\"Initialize example seq2seq agent.\n",
            "\n",
            "        :param opt: options dict generated by parlai.core.params:ParlaiParser\n",
            "        :param shared: optional shared dict with preinitialized model params\n",
            "        \"\"\"\n",
            "        super().__init__(opt, shared)\n",
            "\n",
            "        self.id = 'Seq2Seq'\n",
            "\n",
            "        if not shared:\n",
            "            # set up model from scratch\n",
            "            hsz = opt['hiddensize']\n",
            "            nl = opt['numlayers']\n",
            "\n",
            "            # encoder captures the input text\n",
            "            self.encoder = EncoderRNN(len(self.dict), hsz, nl)\n",
            "            # decoder produces our output states\n",
            "            self.decoder = DecoderRNN(len(self.dict), hsz, nl)\n",
            "\n",
            "            if self.use_cuda:  # set in parent class\n",
            "                self.encoder.cuda()\n",
            "                self.decoder.cuda()\n",
            "\n",
            "            if opt.get('numthreads', 1) > 1:\n",
            "                self.encoder.share_memory()\n",
            "                self.decoder.share_memory()\n",
            "        elif 'encoder' in shared:\n",
            "            # copy initialized data from shared table\n",
            "            self.encoder = shared['encoder']\n",
            "            self.decoder = shared['decoder']\n",
            "\n",
            "        # set up the criterion\n",
            "        self.criterion = nn.NLLLoss()\n",
            "\n",
            "        # set up optims for each module\n",
            "        lr = opt['learningrate']\n",
            "        self.optims = {\n",
            "            'encoder': optim.SGD(self.encoder.parameters(), lr=lr),\n",
            "            'decoder': optim.SGD(self.decoder.parameters(), lr=lr),\n",
            "        }\n",
            "\n",
            "        self.longest_label = 1\n",
            "        self.hiddensize = opt['hiddensize']\n",
            "        self.numlayers = opt['numlayers']\n",
            "        self.START = torch.LongTensor([self.START_IDX])\n",
            "        if self.use_cuda:\n",
            "            self.START = self.START.cuda()\n",
            "\n",
            "        self.reset()\n",
            "\n",
            "    def zero_grad(self):\n",
            "        \"\"\"Zero out optimizer.\"\"\"\n",
            "        for optimizer in self.optims.values():\n",
            "            optimizer.zero_grad()\n",
            "\n",
            "    def update_params(self):\n",
            "        \"\"\"Do one optimization step.\"\"\"\n",
            "        for optimizer in self.optims.values():\n",
            "            optimizer.step()\n",
            "\n",
            "    def share(self):\n",
            "        \"\"\"Share internal states.\"\"\"\n",
            "        shared = super().share()\n",
            "        shared['encoder'] = self.encoder\n",
            "        shared['decoder'] = self.decoder\n",
            "        return shared\n",
            "\n",
            "    def v2t(self, vector):\n",
            "        \"\"\"Convert vector to text.\n",
            "\n",
            "        :param vector: tensor of token indices.\n",
            "            1-d tensors will return a string, 2-d will return a list of strings\n",
            "        \"\"\"\n",
            "        if vector.dim() == 1:\n",
            "            output_tokens = []\n",
            "            # Remove the final END_TOKEN that is appended to predictions\n",
            "            for token in vector:\n",
            "                if token == self.END_IDX:\n",
            "                    break\n",
            "                else:\n",
            "                    output_tokens.append(token)\n",
            "            return self.dict.vec2txt(output_tokens)\n",
            "        elif vector.dim() == 2:\n",
            "            return [self.v2t(vector[i]) for i in range(vector.size(0))]\n",
            "        raise RuntimeError('Improper input to v2t with dimensions {}'.format(\n",
            "            vector.size()))\n",
            "\n",
            "    def vectorize(self, *args, **kwargs):\n",
            "        \"\"\"Call vectorize without adding start tokens to labels.\"\"\"\n",
            "        kwargs['add_start'] = False\n",
            "        return super().vectorize(*args, **kwargs)\n",
            "\n",
            "    def train_step(self, batch):\n",
            "        \"\"\"Train model to produce ys given xs.\n",
            "\n",
            "        :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
            "                      version of observations.\n",
            "\n",
            "        Return estimated responses, with teacher forcing on the input sequence\n",
            "        (list of strings of length batchsize).\n",
            "        \"\"\"\n",
            "        xs, ys = batch.text_vec, batch.label_vec\n",
            "        if xs is None:\n",
            "            return\n",
            "        bsz = xs.size(0)\n",
            "        starts = self.START.expand(bsz, 1)  # expand to batch size\n",
            "        loss = 0\n",
            "        self.zero_grad()\n",
            "        self.encoder.train()\n",
            "        self.decoder.train()\n",
            "        target_length = ys.size(1)\n",
            "        # save largest seen label for later\n",
            "        self.longest_label = max(target_length, self.longest_label)\n",
            "\n",
            "        _encoder_output, encoder_hidden = self.encoder(xs)\n",
            "\n",
            "        # Teacher forcing: Feed the target as the next input\n",
            "        y_in = ys.narrow(1, 0, ys.size(1) - 1)\n",
            "        decoder_input = torch.cat([starts, y_in], 1)\n",
            "        decoder_output, decoder_hidden = self.decoder(decoder_input,\n",
            "                                                      encoder_hidden)\n",
            "\n",
            "        scores = decoder_output.view(-1, decoder_output.size(-1))\n",
            "        loss = self.criterion(scores, ys.view(-1))\n",
            "        loss.backward()\n",
            "        self.update_params()\n",
            "\n",
            "        _max_score, predictions = decoder_output.max(2)\n",
            "        return Output(self.v2t(predictions))\n",
            "\n",
            "    def eval_step(self, batch):\n",
            "        \"\"\"Generate a response to the input tokens.\n",
            "\n",
            "        :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
            "                      version of observations.\n",
            "\n",
            "        Return predicted responses (list of strings of length batchsize).\n",
            "        \"\"\"\n",
            "        xs = batch.text_vec\n",
            "        if xs is None:\n",
            "            return\n",
            "        bsz = xs.size(0)\n",
            "        starts = self.START.expand(bsz, 1)  # expand to batch size\n",
            "        # just predict\n",
            "        self.encoder.eval()\n",
            "        self.decoder.eval()\n",
            "        _encoder_output, encoder_hidden = self.encoder(xs)\n",
            "\n",
            "        predictions = []\n",
            "        done = [False for _ in range(bsz)]\n",
            "        total_done = 0\n",
            "        decoder_input = starts\n",
            "        decoder_hidden = encoder_hidden\n",
            "\n",
            "        for _ in range(self.longest_label):\n",
            "            # generate at most longest_label tokens\n",
            "            decoder_output, decoder_hidden = self.decoder(decoder_input,\n",
            "                                                          decoder_hidden)\n",
            "            _max_score, preds = decoder_output.max(2)\n",
            "            predictions.append(preds)\n",
            "            decoder_input = preds  # set input to next step\n",
            "\n",
            "            # check if we've produced the end token\n",
            "            for b in range(bsz):\n",
            "                if not done[b]:\n",
            "                    # only add more tokens for examples that aren't done\n",
            "                    if preds[b].item() == self.END_IDX:\n",
            "                        # if we produced END, we're done\n",
            "                        done[b] = True\n",
            "                        total_done += 1\n",
            "            if total_done == bsz:\n",
            "                # no need to generate any more\n",
            "                break\n",
            "        predictions = torch.cat(predictions, 1)\n",
            "        return Output(self.v2t(predictions))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sHhm_Iyn3RXF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cp /root/ParlAI/parlai/agents/example_seq2seq/__init__.py /root/ParlAI/parlai/agents/rnn_model/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_0YvvAEt3fni",
        "colab_type": "code",
        "outputId": "25c5dbc4-cb76-444d-de52-c5c9dad1d1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /root/ParlAI/parlai/agents/rnn_model/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__init__.py  __pycache__  rnn_model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ian7bIWxT_Cm",
        "colab_type": "code",
        "outputId": "9f8c7dd7-5c56-434e-8c3c-9fd716a478d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2987
        }
      },
      "cell_type": "code",
      "source": [
        "!python ~/ParlAI/examples/interactive.py -mf /tmp/my_rnn_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: None ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: None ]\n",
            "[  model_file: /tmp/my_rnn_model ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Simple RNN Arguments: ] \n",
            "[  hidden_size: 128 ]\n",
            "[  num_layers: 1 ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  betas: (0.9, 0.999) ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  fp16: False ]\n",
            "[  gpu: -1 ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  history_size: -1 ]\n",
            "[  label_truncate: None ]\n",
            "[  learningrate: 1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  no_cuda: False ]\n",
            "[  nus: (0.7,) ]\n",
            "[  optimizer: sgd ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: -1 ]\n",
            "[  update_freq: -1 ]\n",
            "[  use_reply: label ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: None ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "Dictionary: loading dictionary from /tmp/my_rnn_model.dict\n",
            "[ num words =  26 ]\n",
            "[ Using CUDA ]\n",
            "[creating task(s): parlai.agents.local_human.local_human:LocalHumanAgent]\n",
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 32 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: rnn_model ]\n",
            "[  model_file: /tmp/my_rnn_model ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Simple RNN Arguments: ] \n",
            "[  hidden_size: 128 ]\n",
            "[  num_layers: 1 ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  betas: [0.9, 0.999] ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  fp16: False ]\n",
            "[  gpu: -1 ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  history_size: -1 ]\n",
            "[  label_truncate: None ]\n",
            "[  learningrate: 1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  no_cuda: False ]\n",
            "[  nus: [0.7] ]\n",
            "[  optimizer: sgd ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: -1 ]\n",
            "[  update_freq: -1 ]\n",
            "[  use_reply: label ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: /tmp/my_rnn_model.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "Enter Your Message: Bob is Blue.\\nWhat is Bob?\n",
            "[TorchAgent]: __null__\n",
            "Enter Your Message: Traceback (most recent call last):\n",
            "  File \"/root/ParlAI/examples/interactive.py\", line 18, in <module>\n",
            "    interactive(opt, print_parser=parser)\n",
            "  File \"/root/ParlAI/parlai/scripts/interactive.py\", line 64, in interactive\n",
            "    world.parley()\n",
            "  File \"/root/ParlAI/parlai/core/worlds.py\", line 257, in parley\n",
            "    acts[0] = agents[0].act()\n",
            "  File \"/root/ParlAI/parlai/agents/local_human/local_human.py\", line 39, in act\n",
            "    reply_text = input(\"Enter Your Message: \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2tvRZB5bC4-a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2 (Optional) : Language Modeling"
      ]
    },
    {
      "metadata": {
        "id": "-0A0fpV4bs4o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Language is a method of human communication, either spoken or written, consisting of the use of words in a structured and conventional way. In Natural Language Processing, the set of words used is represented using text. In this section we will define what we understand by text, why language modelling (LM) is useful and give some applications of LM :\n"
      ]
    },
    {
      "metadata": {
        "id": "AKUwTHPOb__i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**A text** is a sequence of token. Tokens can be words, characters or group of characters.\n",
        "\n",
        "\n",
        " A word is a meaningful sequence of charaters.\n",
        " \n",
        " Example :\n",
        "                                      \n",
        "    {INDABAX RWANDA} = {IN, DA, BAX,  RWAN, DA}\n",
        " \n",
        "                     = {INDABAX,  , RWANDA}\n",
        "                                              \n",
        "                     ={I, N, D, A, B, A, X,  , R, W, A, N, D, A}\n",
        "  ----------------"
      ]
    },
    {
      "metadata": {
        "id": "PLzMC_ThcLsz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Language modelling**  is a subfield of NLP which goal is to design methods to assign a probability to a text (sequence of tokens) .\n",
        "To illiustrate it , suppose you have a sequence of tokens $(x_1, x_2, x_3, ..., x_n)$, language model estimate probability of this tokens.\n",
        "\n",
        "$$ P(x_1, x_2, x_3, ..., x_n) $$\n",
        "\n",
        "This probability depends of a **vocabulary** which is a set of unique tokens.\n",
        "\n",
        "Let's take an example:\n",
        "We want to determine a probability of a sentence. We can tokenize it as  sequence of words $(x_1, x_2, x_3, ..., x_t)$\n",
        "We can calculate the probability by using the \"Chain rule of Probability\":\n",
        "\n",
        "$ P(x_1, x_2, x_3, ..., x_r)= \\prod_{t=1} ^{r} P(x_t|x_{t-1}, x_{t-2}, ..., x_{1})  $\n",
        "\n",
        "There exists many ways to coumpute probability,\n",
        "it can be done by counting the number of times a sequence of tokens occurs in the dataset, this method is called **Count based language model**\n",
        "\n",
        "Example of dataset:\n",
        "\n",
        "1: 'I am very happy to assist to INDABAX RWANDA '\n",
        "2: ' I am at Kigali Rwanda' \n",
        "3: 'I am really enjoy this summer school'\n",
        "$P( \\text{Rwanda} | \\text{I am at Kigali } ) = \\frac{c(\\text{I am at Kigali } )}{c( \\text{RWANDA})} =  1/2$\n",
        "\n",
        "One limitation of this method appears when we want to model long sentences. In general, the longer the sentence, the lower its probability : it is more likely to see a sequence of tokens like\n",
        "\n",
        "A:  \"I am at Kigali\" in a dataset than\n",
        "\n",
        "B:  \" am at Kigali eating an ice cream at an Italian restauranIt called Sole Lunda somewhere in Remera because I love italian ice cream\". \n",
        "\n",
        "With respect to our previously defined dataset, sequence B is unkwown (since it doesn't appear in the dataset) and the conditional probabilities are as well unknown (We cannot estimate the probability of \"Luna\" given \"\"I am at Kigali eating an ice cream at an Italian restauranIt called Sole\" based on our dataset)\n",
        "\n",
        "To fix  this issue we truncate past to a fixed size window of size $n$ i.e.:\n",
        "\n",
        "$ P(x_1, x_2, x_3, ..., x_r)= \\prod_{t=1} ^{r} P(x_t|x_{t-1}, x_{t-2}, ..., x_{1})  \\approx \\prod_{t=1} ^{r} P(x_t|x_{t-1}, x_{t-2}, ..., x_{t-n})$\n",
        "\n",
        "this method is call **n-gram model**\n",
        "\n",
        "As we said earlier, ** \"n” ** refers to the size of past\n",
        "\n",
        "- Examples:\n",
        "   - Unigram:\n",
        "   \n",
        "       $ P(w_1 , . . . , w_T ) =\\prod_{t=1} ^{T}  P(w_t )$\n",
        "   - Bigram:\n",
        "   \n",
        "     $ P(w_1 , . . . , w_T ) =\\prod_{t=1} ^{T}  P(w_t | w_{t−1} )$\n",
        "   - Trigram:\n",
        "   \n",
        "   $   P(w_1 , . . . , w_T ) =\\prod_{t=1} ^{T}  P(w_t | w_{t−1},  w_{t−2})$\n",
        "   \n",
        "n-grams can be “good enough” in some cases but n-grams cannot capture long term dependencies required\n",
        "to truely model language.\n",
        "Various methods have been developed to overcume this issues. Among them we can **Backoff** and **Interpolation**\n",
        "\n",
        "[more info](http://www.cs.cornell.edu/courses/cs4740/2014sp/lectures/smoothing+backoff.pdf)\n",
        "\n",
        "\n",
        "------------"
      ]
    },
    {
      "metadata": {
        "id": "cfvqCRn8cRNK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Application**\n",
        "\n",
        "- Speech recognition \n",
        "\n",
        "It is  a subfield of Machine Learning where we want to give the ability to a machine to convert spoken language into text. In real life, some sentences may have the same pronunciation : For example, \"Chocolate, I scream\" and \"Chocolate ice scream\" or \"\"Eye eight uh Jerry\" and \"”I ate a cherry\"\". As humans, in a real conversation, we know that the first sentences are more plausible than the seconds:\n",
        "\n",
        " 1) P(”Chocolate, I scream”) < P(”Chocolate ice cream”)\n",
        "  \n",
        " 2) P(”Eye eight uh Jerry\") < P(”I ate a cherry\")\n",
        "  \n",
        "  A machine can acquire this kind of commonsense using an accurate language model that will give it the probabilities of both sentences in order to choose the most likely one\n",
        "- Machine translation\n",
        "\n",
        " It is the process by which a computer software is used to translate a text from one natural language (such as English) to target language (such as French, Kinyrwanda)\n",
        "[more info](https://www.aclweb.org/anthology/D07-1090.pdf). Given a sentence in the source language, we want to know the most probable sentence in a target language such that they have the same meaning. However, multiple sentences in the target language can be grammaticaly correct and have approximately the same meaning as the one in the source language. For example\n",
        "\n",
        "In French \"Jane visite l'Afrique en Septembre\"\n",
        "\n",
        "In English:\n",
        "\n",
        "1) Jane is visiting Africa in September.\n",
        "\n",
        "2) Jane is going to be visiting Africa in September.\n",
        "\n",
        "3) In September, Jane will visit Africa.\n",
        "\n",
        "4) Her African friend welcomed Jane in September.\n",
        "\n",
        "How can a computer choose the correct one ? By using a language model ! As good english speakers, we know that the first translation is more likely than the forth one in this case. A good language model will give the same result\n",
        "  \n",
        "- Optical Character Recognition & Handwriting recognition\n",
        "\n",
        "Optical character recognition is the recognition of printed or written text characters by a computer. This involves photoscanning of the text character-by-character, analysis of the scanned-in image, and then translation of the character image into character codes, such as ASCII, commonly used in data processing.\n",
        "[more info](https://www.youtube.com/watch?v=ZNrteLp_SvY)\n",
        "\n",
        "Again, language models are used to disentangle ambiguities : for example, sometimes it's difficult to see the difference between 0 (zero) and O (the letter) on a handwritten document. A computer can easily confuse \"m0ve fast\" and \"move fast\" and humans too! But as humans we know that the second one is more likely than the first one : \n",
        "\n",
        "  P(”m0ve fast”) < P(”move fast”)\n",
        "  \n",
        " A good language model will give the ability to the computer to output the right sentence\n",
        " \n",
        "- Context sensitive spelling correction\n",
        "\n",
        "It is the task of correcting spelling errors which results in valid words. Our smartphones heavily use language models to do so.\n",
        "\n",
        "Example :\n",
        "\n",
        "  \"Their are problems wit this sentence\" -> \"There are problems with this sentence\""
      ]
    },
    {
      "metadata": {
        "id": "fks0gKntqaTu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Deep learning for Language Modelling**\n",
        "\n",
        "We know that Deep Learning can be very successful for classification and probability estimation. And as we saw earlier, Language modelling is just conditional probability estimation ! Instead of using n-grams and co., can we replace it by a neural network that will take as input the sequence  $x_{t-1}, x_{t-2}, ..., x_{t-n}$ and output $P(x_t|x_{t-1}, x_{t-2}, ..., x_{t-n})$ ? Yes! And fortunately we have a lot of texts available on Internet i.e. a lot of training data !"
      ]
    },
    {
      "metadata": {
        "id": "D0D2fevW3dPu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation of the RNN Language Model"
      ]
    },
    {
      "metadata": {
        "id": "9aseASAC3kF9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal here is to produce a language model based on Shakespear books : after the training, our model is expect to produce sequences of characters that have a similar style as Shakespear work"
      ]
    },
    {
      "metadata": {
        "id": "OtgM5hUPYffX",
        "colab_type": "code",
        "outputId": "07164737-8bea-4870-9cc0-7552bfc7470f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-13 11:54:24--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘shakespeare_input.txt’\n",
            "\n",
            "shakespeare_input.t 100%[===================>]   4.36M  7.59MB/s    in 0.6s    \n",
            "\n",
            "2019-04-13 11:54:25 (7.59 MB/s) - ‘shakespeare_input.txt’ saved [4573338/4573338]\n",
            "\n",
            "sample_data  shakespeare_input.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QadAOrUVcqb3",
        "colab_type": "code",
        "outputId": "0c062a04-82d6-4797-88b5-c84c379376ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 6.4MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nriuehiS3x0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We start by reading the input file and define the set of valid characters"
      ]
    },
    {
      "metadata": {
        "id": "zOd_X6HEY9HC",
        "colab_type": "code",
        "outputId": "141d67c5-4a86-4ec7-ee00-7affb09b031c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "file = unidecode.unidecode(open('shakespeare_input.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "print(\"n_characters : \", n_characters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 4573338\n",
            "n_characters :  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IfJPEFCx36mL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since we will do mini-batch training, we have to create a function that will extract a \"batch\" i.e. a piece of text from our dataset:"
      ]
    },
    {
      "metadata": {
        "id": "mRdLhL08ZYCI",
        "colab_type": "code",
        "outputId": "3910d650-05e7-4a58-fca8-c4e48acf9040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ysician;\n",
            "His antidotes are poison, and he slays\n",
            "Moe than you rob: take wealth and lives together;\n",
            "Do villany, do, since you protest to do't,\n",
            "Like workmen. I'll example you with thievery.\n",
            "The sun's a th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CnBTeNau4HCE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then it is time to create our model : we will use the exact same model as in the previous section (QA)"
      ]
    },
    {
      "metadata": {
        "id": "RlUsTGXvZZhm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s6NGHESg4M-m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We know that our model only understand numbers. We therefore have to convert each character to a fixed number or more generally, a sequence of characters (a string) into a list of numbers"
      ]
    },
    {
      "metadata": {
        "id": "fXoDhfcUdZof",
        "colab_type": "code",
        "outputId": "1a984ac4-3a74-48d0-d911-fe32200b7ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor)\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zWoAtSFr4bRU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on random_chunk, we have to write a function that will extract a mini-batch:"
      ]
    },
    {
      "metadata": {
        "id": "Pm2HMLCdddmL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_training_set():    \n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JoI9_ROX4miq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a code spinnet to evaluate our model assuming that the first characters are \"*prime_str*\""
      ]
    },
    {
      "metadata": {
        "id": "W_P2iFx7diGe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden().to(device)\n",
        "    prime_input = char_tensor(prime_str).to(device)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1].to(device)    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char).to(device)\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mf3Z7Nrbdl_u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Ehw695MdohO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden().to(device)\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].reshape(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.data / chunk_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VloBXyH0d54s",
        "colab_type": "code",
        "outputId": "f7ddbf63-ed73-4b86-da7f-1aaa40da18f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1198
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 10000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 128\n",
        "n_layers = 3\n",
        "lr = 0.005\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder.to(device)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  inputs, labels = random_training_set()\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  loss = train(inputs, labels)       \n",
        "  loss_avg += loss\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "    print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "    all_losses.append(loss_avg / plot_every)\n",
        "    loss_avg = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0m 34s (100 1%) 2.4968]\n",
            "Whean\n",
            "Beusn,\n",
            "Thum heroy nerekal bann thoun mereanswivhalet can coursde thinnd sit fhabs won! me bnin!  \n",
            "\n",
            "[1m 8s (200 2%) 2.3825]\n",
            "Whams thew the rase thjy mades thesterle nakes waI ciicrucs baves, hy\n",
            "chest maltes? ath onhs maenild t \n",
            "\n",
            "[1m 42s (300 3%) 2.0130]\n",
            "What, her is the wan wee thee. Sor hofs siive.\n",
            "\n",
            "FEM:\n",
            "In to to me it the net to of so in the, epre the  \n",
            "\n",
            "[2m 17s (400 4%) 2.0316]\n",
            "Whis sher Anlighth do highitth for teav\n",
            "That be drake sas mand hin me way a frows teat, the she of war \n",
            "\n",
            "[2m 51s (500 5%) 2.3362]\n",
            "Wher a wave are comzanss.\n",
            "\n",
            "BFId sarse that haes, doth.\n",
            "\n",
            "Furrer your mang:\n",
            "Lathou morks your leark, Fro \n",
            "\n",
            "[3m 26s (600 6%) 2.0570]\n",
            "Whis theed fable sursers of this sorn of to uch\n",
            "tor ploness thubere\n",
            "hould for bould worgerarang toor n \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-882517906776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-a62e78e79030>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inp, target)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ZAqSa7Hlewab",
        "colab_type": "code",
        "outputId": "b0898f10-3f39-4cef-d99a-8ad5201e0b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "cell_type": "code",
      "source": [
        " import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f90c5c5b550>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "hW7VKVTdyu7s",
        "colab_type": "code",
        "outputId": "56f74b0e-66eb-4334-d596-c93345daf403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Th', 200, temperature=0.8))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The carses; and vator't,\n",
            "And it loldine: torile's and him and fortone-doodoss that is\n",
            "Thou come I trone, and indinderenes hoth porn to arn mine.\n",
            "\n",
            "ALGUDIMY ONOTE:\n",
            "Covee plat!\n",
            "\n",
            "RITO\n",
            "And to to that my my k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ldSZxXzN-6jc",
        "colab_type": "code",
        "outputId": "43922232-c74e-46e1-efe0-d4e1483ad91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Th', 200, temperature=0.2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mand the mare the mand the mand the rise to songer the come to doth the doth in to songer the the the may the the come to songer the come to songer the the the come.\n",
            "\n",
            "IOLIO:\n",
            "I and so so in the mand \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CvoXJRL0_A-i",
        "colab_type": "code",
        "outputId": "073dbfc5-451c-462b-bfa5-3a1043bf7cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Th', 200, temperature=1.4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thall tade vOnquarge!wo,\n",
            "Theyousgy. I do qundale, it lither\n",
            "Dowhcm extaje i greplyall.\n",
            "Gowoa tal tished isastierim.\n",
            "'Ote, on tunkerex, s.\n",
            "It my your, you wrostiever,\n",
            "And a mist, nless, anl is let'this J\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85giu5Ov_Cc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}